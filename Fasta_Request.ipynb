{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tspacel/sta141bfinal/blob/main/Fasta_Request.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VM5skKAeTluy"
      },
      "source": [
        "To run and modifiy this notebook you must first go to the *File* menu and *save a copy* of it. Once it's done open this copy and follow the steps. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Nhl_qNw4UWX"
      },
      "source": [
        "#Introduction"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def solution(n):\n",
        "  def fib():\n",
        "    last = (0, 1)\n",
        "    while True:\n",
        "      print(last, 0)\n",
        "      yield last[0]\n",
        "      print(last, 1)\n",
        "      last = last[0] + last[1], last[0]\n",
        "      print(last, 2)\n",
        "\n",
        "  gen = fib()\n",
        "  return [next(gen) for _ in range(n)]\n",
        "\n",
        "solution(5)"
      ],
      "metadata": {
        "id": "r4Wj6Y-pG2Xp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0119bbad-3dc4-4bc7-9847-91d881a4638e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(0, 1) 0\n",
            "(0, 1) 1\n",
            "(1, 0) 2\n",
            "(1, 0) 0\n",
            "(1, 0) 1\n",
            "(1, 1) 2\n",
            "(1, 1) 0\n",
            "(1, 1) 1\n",
            "(2, 1) 2\n",
            "(2, 1) 0\n",
            "(2, 1) 1\n",
            "(3, 2) 2\n",
            "(3, 2) 0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 1, 1, 2, 3]"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3sxTews4gwP"
      },
      "source": [
        "**nsdpy** is designed to download, filter and classify big batch of sequences from **NCBI**. nsdpy stqands for NCBI Sequence Downloader.\n",
        "\n",
        "To do so, enter your own parameters in the code cell from the *Parameters* section.\n",
        "\n",
        "Then you can click *Run all* on the *Runtime* menu or run the cells manually. After entering the query and selecting the options you want to use you can go to runtime > 'run all'.\n",
        "\n",
        "You can save a copy of this notebook (file > save a copy ...) to modify to code according to your needs.\n",
        "\n",
        "Once the cells have finished running you can check the report of the search in the ouput of the *main* subsection code cell.  \n",
        "You'll find the donwloaded files on the right side in the *files* section. Open the folders to see and download the files you need.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UzCvikcy0Hq4"
      },
      "source": [
        "###Google drive\n",
        "\n",
        "Run the following code cell if you wish the results to be downloaded as an archive directly in your *google drive*.\n",
        "Otherwise the results will be saved in *files* section of the runtime (on the left of your screen, the little folder symbol).\n",
        "\n",
        "If you don't want the results to be saved directly to your google drive you can delete this cell or just run all the other cells manualy as this cell only needs to be run if you wish the results to be sent to your *google drive*.\n",
        "\n",
        "\n",
        "Note that the runtime a *google colab* is available for 12 hours maximum, after this time the files saved in the *files* section will be deleted.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vem5KxwNntQD"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "OjylQUMN0Fnx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "972ec9f6-19b5-4225-f98b-169af1f150c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "googledrive = 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGp1DjIK5HBV"
      },
      "source": [
        "##Parameters\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6oPebzwXOyLW"
      },
      "source": [
        "Change the value of the parameters in the following code cell to enter your own parameters. \n",
        "Read the comments in green to know what the parameters correspond to and how to  use them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "OpwR8v0e7Go9"
      },
      "outputs": [],
      "source": [
        "from pickle import TRUE\n",
        "###Enter your request to NCBI wrapped in quotes\n",
        "##example:\"COX1[Title]\"\n",
        "\n",
        "request = 'COX1[Title] AND Mus musculus[ORGN]'\n",
        "\n",
        "\n",
        "###if you wish to provide an Api key enter your API key inside the single quotes\n",
        "#otherwise leave it like this\n",
        "\n",
        "apikey = '6e91886e1c85bd23416dbc931692ffa09508'\n",
        "\n",
        "\n",
        "###if you want the resluts to be classified on particular taxonomic levels:\n",
        "# 1 for kingdoms, \n",
        "# 0 for phylums, \n",
        "# \"s\" for species, \n",
        "#or a list of the levels you wish, for example: [\"Deuterostomia\", \"Protostomia\"]\n",
        "\n",
        "classif = 3\n",
        "\n",
        "\n",
        "### True if you want to download the fasta files containing the coding sequences\n",
        "\n",
        "cds = True\n",
        "\n",
        "\n",
        "####If you want this option to work you need to use the above cds option as well (True)\n",
        "###Enter the name of the gene you want to filter, you can use regular expression\n",
        "##example: [\"COX1\", \"CO[1I]\"] the search is case insensitive\n",
        "\n",
        "filter = [] \n",
        "\n",
        "\n",
        "### True if you want a to get a text file with the accession version numbers\n",
        "#and their corresponding TaxIDs, if not leave \"\".\n",
        "\n",
        "taxids = True\n",
        "\n",
        "\n",
        "### True if you want to have the results written in Tab-separated value format (.tsv)\n",
        "\n",
        "tsv = False\n",
        "\n",
        "\n",
        "### True if you want the taxonomic information added to the information line\n",
        "\n",
        "information = False\n",
        "\n",
        "\n",
        "### enter the file path(s) if you want to add some taxa names from one or more\n",
        "#text file as follow: ['path/to/file', 'path/to/file2']\n",
        "\n",
        "taxa_lists = []\n",
        "\n",
        "\n",
        "### True is you want more output comments \n",
        "\n",
        "verbose = False\n",
        "\n",
        "\n",
        "### True if you want no output comments\n",
        "\n",
        "quiet = False\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Hb1XrdXqG2Kn"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9nFtGa_0QQdM"
      },
      "source": [
        "Once you entered your parameters you can just go to the *Runtime* menu and click *Run all*.\n",
        "\n",
        "Note that if you haven't run or deleted the *Google drive* cell before the notebook will wait for you ton enter the confirmation code required to mount your *google drive* to the notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m1cckejAgyCl"
      },
      "source": [
        "##Run Me!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0amOqZpg4_PG"
      },
      "source": [
        "###functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "BwpZHFcOW1Ph"
      },
      "outputs": [],
      "source": [
        "import requests             #https://requests.readthedocs.io/en/master/\n",
        "import os\n",
        "import re\n",
        "import csv\n",
        "from collections import Counter\n",
        "\n",
        "\n",
        "def download(parameters, address):\n",
        "    ##send requests to the API until getting a result\n",
        "    connect = 0\n",
        "    while True:\n",
        "        try:\n",
        "            result = requests.get(address, params = parameters, timeout = 60)\n",
        "            break\n",
        "        except requests.exceptions.HTTPError as errh:\n",
        "            print(\"Http Error:\", errh)\n",
        "            return(1)\n",
        "\n",
        "        except requests.exceptions.Timeout as to:\n",
        "            print(f'Connection Timed out\\n{to}')\n",
        "            continue\n",
        "\n",
        "        except requests.exceptions.ConnectionError as _:\n",
        "            if connect == 1:\n",
        "                continue\n",
        "            elif connect == 0:\n",
        "                connect = 1\n",
        "                print(f'Connection error (please reconnect)\\n ')\n",
        "                continue\n",
        "\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f'An exception occurred:\\n{e}')\n",
        "            continue\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "def esearchquery(QUERY):\n",
        "    ##unpack QUERY:\n",
        "    (query, api_key) = QUERY\n",
        "\n",
        "    ##build api address\n",
        "    esearchaddress = 'https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi'\n",
        "    #parameters\n",
        "    parameters = {}\n",
        "    if api_key:\n",
        "        parameters[\"api_key\"] = str(api_key)\n",
        "    parameters[\"db\"] = \"nucleotide\"\n",
        "    parameters[\"idtype\"] = \"acc\"\n",
        "    parameters[\"retmode\"] = \"json\"\n",
        "    parameters[\"retmax\"] = \"0\"\n",
        "    parameters[\"usehistory\"] = \"y\"    \n",
        "    #user's query\n",
        "    parameters[\"term\"] = query\n",
        "    \n",
        "    ###send request to the API\n",
        "    y = download(parameters, esearchaddress)\n",
        "    if y == 1:\n",
        "        return ({\"error\": \"wrong address for esearch\"})  \n",
        "    return (y.json())\n",
        "\n",
        "\n",
        "def taxids(params, path, OPTIONS=None):\n",
        "\n",
        "    if OPTIONS is None:\n",
        "        OPTIONS = (\"\",\"\",\"\",\"\",\"\",\"\")\n",
        "\n",
        "    ##unpack parameters\n",
        "    (querykey, webenv, count) = params\n",
        "    (verb, _, _, fileoutput, _, _) = OPTIONS\n",
        "\n",
        "    ##retreive the taxids sending batches of accession numbers to esummary\n",
        "    retmax = 100\n",
        "    dict_ids = {}\n",
        "    taxid = ''\n",
        "    seqnb = ''\n",
        "\n",
        "    if count % retmax == 0:\n",
        "        nb = count//retmax\n",
        "    else: \n",
        "        nb = (count//retmax) + 1\n",
        "    for x in range(nb):\n",
        "        ##build the API address\n",
        "        esummaryaddress = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esummary.fcgi\"\n",
        "        #parameters \n",
        "        parameters = {}\n",
        "        parameters['db'] = \"taxonomy\"\n",
        "        parameters['query_key'] = querykey\n",
        "        parameters['WebEnv'] = webenv\n",
        "        parameters['retstart'] = str(x * retmax)\n",
        "        parameters['retmax'] = str(retmax)\n",
        "        parameters['rettype'] = \"uilist\"\n",
        "        parameters['retmode'] = \"text\"\n",
        "        result = download(parameters, esummaryaddress)\n",
        "\n",
        "        #comments\n",
        "        if verb and verb > 1:\n",
        "            ret = parameters['retstart']\n",
        "            print(f'{round(((int(ret) + 100)/count)*100, 1)} %  of the TaxIDs downloaded')\n",
        "\n",
        "        ###extract the TaxIDs and accession numbers (record in text file and in dict_ids)\n",
        "        f = result.text.splitlines()\n",
        "        for line in f:\n",
        "            if len(line.split('<DocSum>')) > 1:\n",
        "                taxid = ''\n",
        "                seqnb = ''\n",
        "            else:\n",
        "                try:\n",
        "                    version = line.split('<Item Name=\"AccessionVersion\" Type=\"String\">', 1)[1]\n",
        "                    seqnb = version.split(\"<\")[0].strip()\n",
        "                except IndexError:\n",
        "                    pass\n",
        "\n",
        "                TaxId = line.split('<Item Name=\"TaxId\" Type=\"Integer\">', 1)\n",
        "                if len(TaxId) > 1:\n",
        "                    taxid = TaxId[1].split(\"<\")[0].strip()\n",
        "                \n",
        "                if seqnb:\n",
        "                    dict_ids[seqnb] = taxid \n",
        "\n",
        "    if fileoutput:\n",
        "        ##filename\n",
        "        filename = \"TaxIDs.txt\"\n",
        "        ##path to filename\n",
        "        path = path + \"/\" + filename\n",
        "        with open(path, 'a') as summary:\n",
        "            [summary.write(f'{key}  {value}\\n') for key, value in dict_ids.items()]\n",
        "\n",
        "    return dict_ids\n",
        "\n",
        "\n",
        "def dispatch(lineage, classif):\n",
        "    ###Phylums\n",
        "    Plantae = ['Chlorophyta', 'Charophyta', 'Bryophyta', 'Marchantiophyta', 'Lycopodiophyta', 'Ophioglossophyta', 'Pteridophyta',\\\n",
        "    'Cycadophyta', 'Ginkgophyta', 'Gnetophyta', 'Pinophyta', 'Magnoliophyta', 'Equisetidae', 'Psilophyta', 'Bacillariophyta',\\\n",
        "    'Cyanidiophyta', 'Glaucophyta', 'Prasinophyceae','Rhodophyta']\n",
        "    Fungi = ['Chytridiomycota', 'Zygomycota', 'Ascomycota', 'Basidiomycota', 'Glomeromycota']\n",
        "    Metazoa = ['Acanthocephala', 'Acoelomorpha', 'Annelida', 'Arthropoda', 'Brachiopoda', 'Ectoprocta', 'Bryozoa', 'Chaetognatha',\\\n",
        "    'Chordata', 'Cnidaria', 'Ctenophora', 'Cycliophora', 'Echinodermata', 'Echiura', 'Entoprocta', 'Gastrotricha', 'Gnathostomulida',\\\n",
        "    'Hemichordata', 'Kinorhyncha', 'Loricifera', 'Micrognathozoa', 'Mollusca', 'Nematoda', 'Nematomorpha', 'Nemertea', 'Onychophora'\\\n",
        "    'Orthonectida', 'Phoronida', 'Placozoa', 'Plathelminthes', 'Porifera', 'Priapulida', 'Rhombozoa', 'Rotifera', 'Sipuncula',\\\n",
        "    'Tardigrada', 'Xenoturbella']\n",
        "\n",
        "    ##no option selected\n",
        "    if classif == 3 or classif == 2:\n",
        "        return \"sequences\"\n",
        "\n",
        "    ##user gave a list of taxonomic levels\n",
        "    if isinstance(classif, list):\n",
        "        try:\n",
        "            other = [rank for rank in lineage if rank in classif][0]\n",
        "        except IndexError:\n",
        "            other = \"OTHERS\"\n",
        "        return other\n",
        "    ##phylums\n",
        "    if classif == 0:\n",
        "        try:\n",
        "            Phylum = [phy for phy in lineage if phy in Metazoa or phy in Fungi or phy in Plantae][0]\n",
        "        except IndexError:\n",
        "            Phylum = 'OTHERS'\n",
        "        return Phylum\n",
        "    ##kingdoms\n",
        "    if classif == 1:\n",
        "        if 'Metazoa' in lineage or len(list(set(lineage) & set(Metazoa))) > 0:\n",
        "            kingdom = \"METAZOA\"\n",
        "        elif \"Viridiplantae\" in lineage or len(list(set(lineage) & set(Plantae))) > 0:\n",
        "            kingdom = \"PLANTAE\" \n",
        "        elif \"Fungi\" in  lineage or len(list(set(lineage) & set(Fungi))) > 0:\n",
        "            kingdom = \"FUNGI\" \n",
        "        else:\n",
        "            kingdom = \"OTHERS\"\n",
        "        return kingdom\n",
        "    ##if the users choose to make groupe n rank higher than species (classif >= 3)\n",
        "    classif = -(int(classif) - 2)\n",
        "    try:\n",
        "        rank = lineage[classif]\n",
        "    except IndexError:\n",
        "        rank = \"OTHERS\"\n",
        "    return rank\n",
        "\n",
        "\n",
        "#query taxonomy with efetch, returns a dict with taxid as key and info in a dict as value\n",
        "def completetaxo(idlist, QUERY, OPTIONS):\n",
        "\n",
        "    ##unpack parameters\n",
        "    (_, api_key) = QUERY\n",
        "    (verb, _, classif, _, _, _) = OPTIONS\n",
        "\n",
        "    if verb and verb > 0:\n",
        "        print(\"retrieving taxonomy...\")\n",
        "\n",
        "    ##dictionnary that will be returned\n",
        "    data = {}\n",
        "    idlist = [i.split(\".\")[0] for i in idlist]\n",
        "    ##retreive the taxonomy sending batches of TaxIds to efetch\n",
        "    #number of TaxIds to be sent to the API at once\n",
        "    retmax = 100\n",
        "    count = len(idlist)\n",
        "    if count % retmax == 0:                                    \n",
        "        nb = count//retmax\n",
        "    else: \n",
        "        nb = (count//retmax) + 1\n",
        "\n",
        "    for x in range(nb):\n",
        "        ##slice the idlist\n",
        "        retstart = x * retmax\n",
        "        idsublist = idlist[retstart:(retstart+retmax)]\n",
        "        idsublist = ','.join(idsublist)\n",
        "\n",
        "        ##build API address\n",
        "        efetchaddress = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi\"\n",
        "        parameters = {}\n",
        "        #parameters \n",
        "        parameters['db'] = \"taxonomy\"\n",
        "        parameters['id'] = idsublist\n",
        "        if api_key:\n",
        "            parameters['api_key'] = api_key\n",
        "\n",
        "        ##loop until download is correct\n",
        "        result = download(parameters, efetchaddress)\n",
        "\n",
        "        #comments\n",
        "        if verb > 1:\n",
        "            print(f'{round((int(retstart)/count)*100, 1)} % of the taxonomy found')\n",
        "\n",
        "        ##analyse the results from efetch\n",
        "        result = result.text.split('</Taxon>\\n<Taxon>')\n",
        "\n",
        "        for seq in result:\n",
        "            dicttemp = {}\n",
        "            try:\n",
        "                TaxId, _ = seq.split('</TaxId>', 1)\n",
        "                _, TaxId = TaxId.split('<TaxId>', 1)\n",
        "                TaxId = TaxId.strip()    \n",
        "            except ValueError:\n",
        "                TaxId = 'not found'\n",
        "\n",
        "            #check if the taxonomy for a given TaxId is already in memory\n",
        "            if TaxId in data.keys():\n",
        "                continue\n",
        "            \n",
        "            try:\n",
        "                Name , _ = seq.split('</ScientificName>', 1)\n",
        "                _, Name = Name.split('<ScientificName>', 1)   \n",
        "            except ValueError:\n",
        "                Name = 'not found'\n",
        "            dicttemp['Name'] = Name\n",
        "\n",
        "            try:\n",
        "                Lineage , _ = seq.split('</Lineage>', 1)\n",
        "                _, Lineage = Lineage.split('<Lineage>', 1)    \n",
        "            except ValueError:\n",
        "                Lineage = 'not found'\n",
        "            lineage = Lineage.split('; ')\n",
        "            dicttemp['Lineage'] = lineage\n",
        "\n",
        "            ##dispatch\n",
        "            dicttemp['dispatch'] = dispatch(lineage, classif)\n",
        "\n",
        "            data[TaxId] = dicttemp\n",
        "\n",
        "    #comments\n",
        "    if verb and verb > 0:\n",
        "        print(f'number of taxids:\\t{len(data.keys())}')\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "## Download the CDS fasta files by batch of 'retmax' for the seq access found by an esearch request returning a querykey and a webenv variable\n",
        "def cds_fasta(path, dict_ids, dict_taxo, QUERY, list_of_ids, OPTIONS=None):\n",
        "\n",
        "    if OPTIONS is None:\n",
        "        OPTIONS = (\"\",\"\",\"\",\"\",\"\",\"\")\n",
        "    \n",
        "    ## Unpack parameters\n",
        "    (_, api_key) = QUERY\n",
        "    (verb, genelist, classif, _, tsv, information)= OPTIONS\n",
        "\n",
        "    # Comment:\n",
        "    if verb and verb > 0:\n",
        "        print(\"Downloading the CDS fasta files...\")\n",
        "\n",
        "    # Number of accession numbers to be sent at each API query\n",
        "    retmax = 200    \n",
        "    # List of accession number for wich a gene is found or the file has been retrieve if no gene filter:\n",
        "    found = []\n",
        "    count = len(list_of_ids)\n",
        "\n",
        "    if count % retmax == 0:\n",
        "        nb = count//retmax\n",
        "    else: \n",
        "        nb = (count//retmax) + 1\n",
        "\n",
        "    for x in range(nb):\n",
        "        ## Split the list of ids\n",
        "        ids = list_of_ids[x * retmax : (x * retmax) + retmax]\n",
        "        ## Check that id parameters is not empty\n",
        "        ids = [i for i in ids if i]\n",
        "        ## Build API address\n",
        "        efetchaddress = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi\"\n",
        "        parameters = {}\n",
        "        # Parameters \n",
        "        parameters['id'] = \",\".join(ids)\n",
        "        parameters['db'] = \"nuccore\"\n",
        "        if api_key:\n",
        "            parameters[\"api_key\"] = api_key\n",
        "        parameters['rettype'] = \"fasta_cds_na\"\n",
        "        parameters['retmode'] = \"text\"\n",
        "\n",
        "        ## Download\n",
        "        raw_result = download(parameters, efetchaddress)\n",
        "        raw_result = raw_result.text\n",
        "\n",
        "        ## Extract available information\n",
        "        if not information and not genelist and classif == 3:\n",
        "            result_fasta = raw_result.split(\">lcl|\")[1:]\n",
        "            sublist = [r.split(\"_cds\")[0] for r in result_fasta]\n",
        "  \n",
        "        ##analyse the results     \n",
        "        sublist = extract(path, raw_result, dict_ids, dict_taxo, OPTIONS, verb)\n",
        "\n",
        "        found = found + sublist\n",
        "        #comments\n",
        "        if verb > 1:\n",
        "            start = parameters['retmax']\n",
        "            print(f'{round(((x * int(start) + 100)/count)*100, 1)} %  of the CDS fasta files downloaded')\n",
        "\n",
        "    return found\n",
        "\n",
        "\n",
        "def subextract(seq, path, dict_ids, dict_taxo, genelist, OPTIONS=None):\n",
        "\n",
        "    if OPTIONS is None:\n",
        "        OPTIONS = (\"\",\"\",\"\",\"\",\"\",\"\")\n",
        "\n",
        "    (_, _, classif, _, tsv, information) = OPTIONS\n",
        "\n",
        "\n",
        "    ###find gene in a seq and write to ouput file\n",
        "    ##extract accession number\n",
        "    try:\n",
        "        key = seq.split(\">lcl|\")[1].split(\"_cds\")[0]\n",
        "    except IndexError:\n",
        "        return\n",
        "\n",
        "    ##extract SeqID\n",
        "    try: \n",
        "        SeqID = seq.split(\">lcl|\")[1].split(\" [\")[0]\n",
        "    except IndexError:\n",
        "        return\n",
        "\n",
        "    ##build id_line (retrieve info)\n",
        "    try:\n",
        "        TaxId = dict_ids[key]\n",
        "    except KeyError:\n",
        "        return \n",
        "\n",
        "    ## Extract info\n",
        "    #Lineage\n",
        "    try:                                                       \n",
        "        Lineage = dict_taxo[TaxId]['Lineage']\n",
        "    except KeyError:\n",
        "        Lineage = \"no info\"\n",
        "\n",
        "    #Name\n",
        "    try:\n",
        "        Name = dict_taxo[TaxId]['Name']  \n",
        "    except KeyError:\n",
        "        Name = \"no info\"\n",
        "\n",
        "    #dispatch\n",
        "    try:\n",
        "        dispatch = dict_taxo[TaxId]['dispatch']\n",
        "    except KeyError:\n",
        "        dispatch = \"others\"\n",
        "    if classif == 3:\n",
        "        dispatch = \"sequences\"\n",
        "\n",
        "    ##check if genes\n",
        "    check = [1 for reg_exp in genelist if len(re.split(reg_exp, seq, flags=re.IGNORECASE)) > 1]\n",
        "    if 1 in check or not genelist:\n",
        "        ##get the Sequence\n",
        "        _, dna = seq.split('\\n', 1)\n",
        "        \n",
        "        # if dict_taxo and information:\n",
        "        if information:\n",
        "            Lineage = \", \".join(Lineage)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \n",
        "            info_line = Name + '-' + SeqID + ' | ' + TaxId + ' | ' + Lineage \n",
        "        else:\n",
        "            info_line = seq.split('\\n', 1)[0]\n",
        "\n",
        "        # Make only one > and \\n\n",
        "        info_line = '>' + info_line.lstrip(\">\")\n",
        "        info_line = info_line.rstrip(\"\\n\") + \"\\n\"\n",
        "\n",
        "        ## Create folders is tsv is selected\n",
        "        if tsv:\n",
        "            if not os.path.exists(path + \"/fasta\"):\n",
        "                os.makedirs(path + \"/fasta\")\n",
        "            if not os.path.exists(path + \"/tsv\"):\n",
        "                os.makedirs(path + \"/tsv\")\n",
        "            # Create paths\n",
        "            fasta_file = path + \"/fasta/\" + dispatch + \".fasta\"\n",
        "            tsv_file = path + \"/tsv/\" + dispatch + \".tsv\"\n",
        "        else:\n",
        "            fasta_file = path + \"/\" + dispatch + \".fasta\"\n",
        "            tsv_file = path + \"/\" + dispatch + \".tsv\"\n",
        "\n",
        "        # write fasta file\n",
        "        with open(fasta_file, 'a') as new:\n",
        "            new.write(str(info_line))\n",
        "            new.write(str(dna).rstrip(\"\\n\") + \"\\n\") \n",
        "\n",
        "        # write tsv file\n",
        "        if tsv:\n",
        "            # Format dna sequence \n",
        "            dna = \"\".join(dna.split(\"\\n\"))\n",
        "\n",
        "            # write .tsv file\n",
        "            data = (Name, SeqID, TaxId, Lineage, dna)\n",
        "            tsv_file_writer(tsv_file, data, OPTIONS)\n",
        "\n",
        "        return key\n",
        "\n",
        "    else:\n",
        "        return\n",
        "\n",
        "\n",
        "def extract(path, text, dict_ids, dict_taxo, OPTIONS=None, verb=\"\"):\n",
        "\n",
        "    # unpack OPTIONS\n",
        "    (verb, genelist, classif, _, tsv, information)= OPTIONS\n",
        "\n",
        "    # Comments\n",
        "    if verb and verb > 1:\n",
        "        print('analyzing the results...')\n",
        "    \n",
        "    found = []\n",
        "\n",
        "    # Extract genes to filter\n",
        "    genelist = [ \"=\" + gene + \"]\" for gene in genelist]  \n",
        "\n",
        "    seq = ''\n",
        "    text = text.splitlines()\n",
        "\n",
        "    if not text:\n",
        "        return []\n",
        "\n",
        "    for line in text:\n",
        "        if len(line.split(\">lcl|\")) > 1:\n",
        "            if seq:\n",
        "                try:\n",
        "                    result = subextract(seq, path, dict_ids, dict_taxo, genelist, OPTIONS)\n",
        "                    if result:\n",
        "                        found.append(result)\n",
        "                except:\n",
        "                    pass\n",
        "            seq = str(line)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \n",
        "        else:\n",
        "            seq = seq + '\\n' + line\n",
        "\n",
        "    return found\n",
        "\n",
        "\n",
        "def fasta(path, dict_ids, dict_taxo, QUERY, list_of_ids, OPTIONS=None):\n",
        "\n",
        "    if OPTIONS is None:\n",
        "        OPTIONS = (\"\",\"\",\"\",\"\",\"\",\"\")\n",
        "    \n",
        "    ## Unpack parameters\n",
        "    (_, api_key) = QUERY\n",
        "    (verb, _, classif, _, tsv, information)= OPTIONS\n",
        "\n",
        "    if verb and verb > 0:\n",
        "        print(\"Downloading the fasta files...\")\n",
        "\n",
        "    retmax = 200    ## Number of sequences per request to the API\n",
        "    keys = []\n",
        "    count = len(list_of_ids)\n",
        "\n",
        "    if count % retmax == 0:\n",
        "        nb = count//retmax\n",
        "    else: \n",
        "        nb = (count//retmax) + 1\n",
        "\n",
        "    for x in range(nb):\n",
        "        ## Split the list of ids\n",
        "        ids = list_of_ids[x * retmax : (x * retmax) + retmax]\n",
        "        ## Check that id parameters is not empty\n",
        "        ids = [i for i in ids if i]\n",
        "        ## Build API address\n",
        "        efetchaddress = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi\"\n",
        "        parameters = {}\n",
        "        # Parameters \n",
        "        parameters['db'] = \"nuccore\"\n",
        "        parameters['id'] = \",\".join(ids)\n",
        "        if api_key:\n",
        "            parameters[\"api_key\"] = api_key\n",
        "        parameters['rettype'] = \"fasta\"\n",
        "        parameters['retmode'] = \"text\"\n",
        "\n",
        "        ## Download\n",
        "        raw_result = download(parameters, efetchaddress)\n",
        "        raw_result = raw_result.text\n",
        "\n",
        "        ## Extract available informations\n",
        "        result = raw_result.split('>')\n",
        "\n",
        "        ## Store analyzed Accession version numbers\n",
        "        for seq in result:\n",
        "            try:\n",
        "                id_line, dna = seq.split('\\n', 1)\n",
        "            except ValueError:\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                key = id_line.split()[0]\n",
        "            except IndexError:\n",
        "                continue\n",
        "            \n",
        "            #from dict_ids\n",
        "            try:\n",
        "                taxid = dict_ids[key]\n",
        "            except KeyError:\n",
        "                taxid = 'not found'\n",
        "            \n",
        "            # from dict_taxo\n",
        "            try:\n",
        "                lineage = dict_taxo[taxid]['Lineage']\n",
        "                lineage = \", \".join(lineage)\n",
        "            except KeyError:\n",
        "                lineage = 'not found'\n",
        "            \n",
        "            try:\n",
        "                name = dict_taxo[taxid]['Name']\n",
        "            except KeyError:\n",
        "                name = 'not found'\n",
        "            \n",
        "            try:\n",
        "                dispatch = dict_taxo[taxid]['dispatch']\n",
        "            except KeyError:\n",
        "                name = 'others'\n",
        "            \n",
        "            if classif == 3:\n",
        "                dispatch = \"sequences\"\n",
        "            \n",
        "            data = (name, key, taxid, lineage, dna)\n",
        "\n",
        "            # Create folders for .tsv files and .fasta files\n",
        "            if tsv:\n",
        "                if not os.path.exists(path + \"/fasta\"):\n",
        "                    os.makedirs(path + \"/fasta\")\n",
        "                if not os.path.exists(path + \"/tsv\"):\n",
        "                    os.makedirs(path + \"/tsv\")\n",
        "                # Create paths\n",
        "                fasta_file = path + \"/fasta/\" + dispatch + \".fasta\"\n",
        "                tsv_file = path + \"/tsv/\" + dispatch + \".tsv\"\n",
        "            else:\n",
        "                fasta_file = path + \"/\" + dispatch + \".fasta\"\n",
        "                tsv_file = path + \"/\" + dispatch + \".tsv\"\n",
        "\n",
        "\n",
        "            # Write fasta file\n",
        "            if information: \n",
        "                id_line = name + \"-\" + key + \" | \" + taxid + \" | \" + lineage + \" | \" + id_line\n",
        "            with open(fasta_file, 'a') as f:\n",
        "                f.write(f\">{id_line}\\n\")\n",
        "                f.write(f\"{dna}\\n\")\n",
        "\n",
        "            if tsv:\n",
        "                tsv_file_writer(tsv_file, data, OPTIONS)\n",
        "\n",
        "            keys.append(key)\n",
        "\n",
        "        if verb > 1:\n",
        "            start = (x * retmax) + retmax\n",
        "            print(f'{round((start / count) * 100, 1)} %  of the fasta files downloaded')\n",
        "\n",
        "    return keys\n",
        "\n",
        "\n",
        "def duplicates(listofaccess, path):\n",
        "    filename = path + \"/duplicates.txt\"\n",
        "    count = Counter(listofaccess)\n",
        "    count = dict(count)\n",
        "    nb = 0\n",
        "    for key, value in count.items():\n",
        "        if value > 1:\n",
        "            nb += 1\n",
        "            with open(filename, \"a\") as f:\n",
        "                f.write(f\"{key}   {value}\\n\")\n",
        "    return(nb)\n",
        "\n",
        "\n",
        "def taxo(path, list_of_ids, dict_ids, QUERY, dict_taxo=None, OPTIONS=None):\n",
        "\n",
        "    if OPTIONS is None:\n",
        "        OPTIONS = (\"\",\"\",\"\",\"\",\"\",\"\")\n",
        "\n",
        "    if len(list_of_ids) < 1:\n",
        "        return ([],[])\n",
        "\n",
        "    ##unpack params\n",
        "    (verb, genelist, classif, _, tsv, information) = OPTIONS\n",
        "    (_, api_key) = QUERY\n",
        "\n",
        "    ##build output unique filename\n",
        "    notfound = path + \"/notfound.txt\"\n",
        "\n",
        "    #format the expression to be found in 'gene' (from gene and CDS section of the gb file)\n",
        "    if genelist:\n",
        "        genelist = ['[; \"()]+' + gene + '[\"; ()]+' for gene in genelist]\n",
        "\n",
        "    #comments\n",
        "    if verb and verb > 0:\n",
        "        print(\"Downloading the GenBank files...\")\n",
        "\n",
        "    remain = []         #accessions not downloaded from previous iteration\n",
        "    analysed = []       #accessions successfully donwnloaded and found in the gb file\n",
        "    genefound = []      ##accessions with some cds found or matching the filter if filter(s)\n",
        "    count = len(list_of_ids)\n",
        "    retmax = 10\n",
        "    if count % retmax == 0:\n",
        "        nb = count//retmax\n",
        "    else: \n",
        "        nb = (count//retmax) + 1\n",
        "\n",
        "    for x in range(nb):\n",
        "        ###################  API CALL  ##################\n",
        "        ##slice the list of ids passed to the function\n",
        "        ids = list_of_ids[x * retmax:(x+1) * retmax]\n",
        "        ##if some ids haven't been dl at the last call add them to this call\n",
        "        if remain:\n",
        "            ids = ids + remain\n",
        "        ids1 = \",\".join(ids)\n",
        "        retstart = str(x * retmax)\n",
        "\n",
        "        ##build API address\n",
        "        efetchaddress = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi\"\n",
        "        parameters = {}\n",
        "        #parameters \n",
        "        parameters['db'] = \"nuccore\"\n",
        "        parameters['id'] = ids1\n",
        "        parameters['rettype'] = \"gb\"\n",
        "        parameters['retmode'] = \"text\"\n",
        "        if api_key:\n",
        "            parameters[\"api_key\"] = api_key\n",
        "        \n",
        "        ##loop until dl is correct\n",
        "        result = download(parameters, efetchaddress)\n",
        "        result = result.text\n",
        "\n",
        "\n",
        "        ########################################################################\n",
        "        #######################   ANALYZING RESULTS   ##########################\n",
        "        ########################################################################\n",
        "\n",
        "        result = result.split('\\n//')\n",
        "        ###extract the CDS for each asscession version number\n",
        "        accessionlist = []\n",
        "        for i, seq in enumerate(result[:-1]):\n",
        "            ##search in CDS\n",
        "            listCDS, dna = genbankfields(seq, genelist)\n",
        "\n",
        "            for dictCDS in listCDS:\n",
        "                if dictCDS:\n",
        "                    accessionlist.append(dictCDS[\"version\"])\n",
        "                    #select the gene if genes to select\n",
        "                    if \"gene\" in dictCDS.keys():\n",
        "                        ##genefound\n",
        "                        genefound.append(dictCDS[\"version\"])\n",
        "                        ###path\n",
        "                        taxo = dictCDS[\"taxo\"]\n",
        "                        filename = dispatch(taxo, classif)\n",
        "                        if tsv:\n",
        "                            fasta_file = path + \"/fasta/\" + filename + \".fasta\"\n",
        "                        else:\n",
        "                            fasta_file = path + \"/\" + filename + \".fasta\"\n",
        "                        \n",
        "                        ##information line \n",
        "                        taxo = ', '.join(taxo)\n",
        "                        \n",
        "                        try:\n",
        "                            taxid = dict_ids[dictCDS[\"version\"]]\n",
        "                        except IndexError:\n",
        "                            taxid = \"\"\n",
        "\n",
        "                        if information:\n",
        "                            try:\n",
        "                                name = dict_taxo[taxid]['Name']\n",
        "                            except:\n",
        "                                name = \"not found\"\n",
        "                            info_line = \">\" + name + \"-\" + dictCDS[\"version\"] + \"_cds_\" + dictCDS[\"proteinid\"].strip('\"') + \" | \" + taxid + \" | \" +  \"\".join(taxo).rstrip(\".\")\n",
        "                     \n",
        "                        else:\n",
        "                            info_line = \">\" + dictCDS[\"version\"] + \"_cds_\" + dictCDS[\"proteinid\"].strip('\"') + \" [gene=\" + dictCDS[\"gene\"] + \"] \" + \"[protein=\" + dictCDS[\"proteinid\"] + \"] \" + \\\n",
        "                                \"[location=\" + dictCDS[\"loc\"].strip() + \"] \" + \"[gbkey=CDS] \" + \"[definition=\" + \" \".join(dictCDS[\"definition\"].split(\" \" * 12)).rstrip(\".\") + \"]\"\n",
        "\n",
        "                        ## append to file\n",
        "                        with open(fasta_file, 'a') as a:\n",
        "                            a.write(f\"{info_line}\\n\")\n",
        "                            [a.write(f'{\"\".join(list(dictCDS[\"sequence\"])[i: i + 70]).upper()}\\n') for i in range(0, len(dictCDS[\"sequence\"]), 70)]\n",
        "\n",
        "        remain = list(set(accessionlist) - set(ids))\n",
        "        analysed = analysed + accessionlist\n",
        "\n",
        "        #comments\n",
        "        if verb > 1:\n",
        "            print(f'{round(((int(retstart))/count)*100, 1)} %  of the remaining  analysis done')\n",
        "\n",
        "    if remain:\n",
        "        for number in remain:\n",
        "            with open(notfound, 'a') as nf:\n",
        "                nf.write(f'{number}    Accession number not found in gb files\\n')\n",
        "\n",
        "    #accessions found in bg file wihtout matching gene\n",
        "    nogene = list(set(analysed) - set(genefound))\n",
        "    for number in nogene:\n",
        "        with open(notfound, 'a') as nf:\n",
        "            nf.write(f'{number}    No matching gene found in gb file\\n')\n",
        "\n",
        "    return analysed, genefound\n",
        "            \n",
        "\n",
        "\n",
        "def genbankfields(text, genelist):\n",
        "    ###extract the CDS for each asscession version number\n",
        "    dictfield = {}\n",
        "    listofdict = []\n",
        "    dna = []\n",
        "    ##ACCESSION VERSION NUMBER\n",
        "    try:\n",
        "        version = text.split('VERSION', 1)[1]\n",
        "        version = version.split('\\n', 1)[0]\n",
        "        version = version.split()[0]\n",
        "        version = version.strip()\n",
        "        dictfield[\"version\"] = version\n",
        "    except IndexError:\n",
        "        return listofdict, dna\n",
        "\n",
        "    ##ORGANISM And TAXO\n",
        "    try:\n",
        "        taxo = text.split('ORGANISM', 1)[1]\n",
        "        taxo = taxo.split('REFERENCE')[0]\n",
        "        organism = taxo.splitlines()[0].strip()\n",
        "        taxo = ''.join(taxo.splitlines()[1:]).split(\"; \")\n",
        "        taxo = [t.strip() for t in taxo]\n",
        "    except IndexError:\n",
        "        taxo, organism = \"not found\", \"not found\"\n",
        "    dictfield[\"taxo\"] = taxo\n",
        "    dictfield[\"organism\"] = organism    \n",
        "\n",
        "    ###Definition line (to use if information option is not selected)\n",
        "    try:\n",
        "        definition = text.split('DEFINITION', 1)[1]\n",
        "        definition = definition.split('ACCESSION', 1)[0]\n",
        "    except IndexError:\n",
        "        definition = \"not found\"\n",
        "    dictfield[\"definition\"] = \"\".join(definition.split(\"\\n\"))\n",
        "\n",
        "    ###DNA sequence\n",
        "    try:\n",
        "        dna = text.split(\"ORIGIN\")[1].splitlines()\n",
        "        dna = [d.strip().strip('1234567890') for d in dna]\n",
        "        dna = \"\".join(\"\".join(dna).split())\n",
        "    except IndexError:\n",
        "        dna = []\n",
        "    dictfield[\"dna\"] = dna\n",
        "\n",
        "    ###look for all the CDS, their gene names or product names (== protein) and locations and sequences\n",
        "    ##and protein_id, frame\n",
        "    seqgene = text.split(\"  gene  \")\n",
        "    for seq in seqgene:\n",
        "        dictgene = {}\n",
        "        dictgene = search(dna, dictgene, seq)\n",
        "        seqcds = seq.split(\"  CDS  \")[1:]\n",
        "        for seq1 in seqcds:\n",
        "            dictcds = search(dna, dictfield, seq1)\n",
        "            if genelist:\n",
        "                ##check if target is found\n",
        "                check = [1 for reg in genelist if re.findall(reg, dictcds[\"product\"], flags=re.IGNORECASE) or re.findall(reg, dictcds[\"gene\"], flags=re.IGNORECASE)\\\n",
        "                or re.findall(reg, dictcds[\"note\"], flags=re.IGNORECASE) or re.findall(reg, dictcds[\"genesynonym\"], flags=re.IGNORECASE)]\n",
        "                try:\n",
        "                    if not check:\n",
        "                        if dictgene[\"location\"][0][0] ==  dictcds[\"location\"][0][0] and dictgene[\"location\"][-1][1] ==  dictcds[\"location\"][-1][1]:\n",
        "                            check = [1 for reg in genelist if re.findall(reg, dictgene[\"product\"], flags=re.IGNORECASE) or re.findall(reg, dictgene[\"gene\"], flags=re.IGNORECASE)\\\n",
        "                            or re.findall(reg, dictgene[\"note\"], flags=re.IGNORECASE) or re.findall(reg, dictgene[\"genesynonym\"], flags=re.IGNORECASE)]\n",
        "                except IndexError:\n",
        "                    pass\n",
        "                if check:\n",
        "                    listofdict.append(dictcds)\n",
        "            else:\n",
        "                listofdict.append(dictcds)\n",
        "        \n",
        "    if listofdict:\n",
        "        return listofdict, dna \n",
        "    else:\n",
        "        listofdict.append(dictfield)\n",
        "        return listofdict, dna\n",
        "    \n",
        "\n",
        "def search(dna ,dictentry, s):\n",
        "    dict1 = dict(dictentry)\n",
        "    s = s.split(\"RNA   \")[0].split(\"  misc_feature  \")[0].split(\"  gene  \")[0].split(\"repeat_region\")[0]\\\n",
        "        .split(\"transit_peptide\")[0].split(\"mat_peptide\")[0].split(\"3'UTR\")[0].split(\"5'UTR\")[0]\n",
        "    #Location and sequence\n",
        "    if dna:\n",
        "    ##if join or complement:\n",
        "        try:\n",
        "            loc = s.split(\"/\")[0]\n",
        "            loc1 = loc.split(',')\n",
        "            loc1 = [(int(i.split('..')[0].strip(\"cmpletjoin(><) \\n\")), int(i.split('..')[1].strip(\"cmpletjoin(><) \\n\"))) for i in loc1]\n",
        "            sequence = \"\".join([dna[i[0]:i[1]] for i in loc1])\n",
        "        #else\n",
        "        except IndexError:\n",
        "            sequence = \"\"\n",
        "            loc = \"not found\"\n",
        "            loc1 = \"not found\"\n",
        "        except ValueError:\n",
        "            sequence = \"\"\n",
        "            loc = \"not found\"\n",
        "            loc1 = \"not found\"\n",
        "    else:\n",
        "        sequence = \"\"\n",
        "        loc = \"not found\"\n",
        "        loc1 = \"not found\"\n",
        "    dict1[\"sequence\"] = sequence\n",
        "    dict1[\"location\"] = loc1\n",
        "    dict1[\"loc\"] = \"\".join(loc.split(\"\\n\"))\n",
        "\n",
        "    \n",
        "    #Product\n",
        "    try:\n",
        "        product = s.split('product=\"')[1].split('\"')[0]\n",
        "    except IndexError:\n",
        "        product = \"not found\"\n",
        "    dict1[\"product\"] = product\n",
        "\n",
        "    #protein_id\n",
        "    try: \n",
        "        proteinid = s.split(\"protein_id=\")[1].split()[0]\n",
        "    except IndexError:\n",
        "        proteinid = \"not found\"\n",
        "    dict1[\"proteinid\"] = proteinid\n",
        "\n",
        "    #note\n",
        "    try:\n",
        "        note = s.split(\"note=\")[1].split(\"/\")[0]\n",
        "        note = \"\".join(note.split(\"\\n\"))\n",
        "    except IndexError:\n",
        "        note = \"\"\n",
        "    dict1[\"note\"] = note\n",
        "\n",
        "    #gene\n",
        "    try:\n",
        "        gene = s.split(\"gene=\")[1].split(\"\\n\")[0]\n",
        "    except IndexError:\n",
        "        gene = \"not found\"\n",
        "    dict1[\"gene\"] = gene\n",
        "\n",
        "    #gene_synonym\n",
        "    try:\n",
        "        genesynonym = s.split(\"gene_synonym=\")[1].split(\"/\")[0].strip(\"\\n\")\n",
        "    except IndexError:\n",
        "        genesynonym = \"not found\"\n",
        "    dict1[\"genesynonym\"] = genesynonym\n",
        "\n",
        "    #locus_tag\n",
        "    try:\n",
        "        locustag = s.split(\"locus_tag=\")[1].split()[0]\n",
        "    except IndexError:\n",
        "        locustag = \"not found\"\n",
        "    dict1[\"locustag\"] = locustag\n",
        "\n",
        "    return dict1\n",
        "\n",
        "\n",
        "\n",
        "def tsv_file_writer(path, data, OPTIONS=None):\n",
        "\n",
        "    if OPTIONS is None:\n",
        "        OPTIONS = (\"\",\"\",\"\",\"\",\"\",\"\")\n",
        "\n",
        "    (_, _, _, _, _, information) = OPTIONS   \n",
        "\n",
        "        \n",
        "    # check if the file already exists\n",
        "    if not os.path.exists(path):\n",
        "        with open(path, \"a\") as tsv_to_write:\n",
        "            writer = csv.writer(tsv_to_write, delimiter=\"\\t\")\n",
        "            if information:\n",
        "                writer.writerow(['Name', 'SeqID', 'TaxID', 'Lineage', 'sequence length', 'sequence'])\n",
        "            else:\n",
        "                writer.writerow(['SeqID', 'TaxID', 'sequence length', 'sequence'])\n",
        "\n",
        "    \n",
        "    # unpack data\n",
        "    (name, seqid, taxid, lineage, dna) = data\n",
        "\n",
        "    dna = \"\".join(dna.split(\"\\n\"))\n",
        "\n",
        "    # write data\n",
        "    with open(path, \"a\") as outtsv:\n",
        "        writer = csv.writer(outtsv, delimiter='\\t')\n",
        "        if information:\n",
        "            writer.writerow([name, seqid, taxid, lineage, len(dna), dna])\n",
        "        else:\n",
        "            writer.writerow([seqid, taxid, len(dna), dna])\n",
        "\n",
        "    return\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J8FoCX9HW1yD"
      },
      "source": [
        "### main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wInNoFOn0AYn",
        "outputId": "35b3d18f-dd0a-4484-b308-20beefa12bc9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "retrieving results for COX1[Title] AND Mus musculus[ORGN]) ....\n",
            "Number of results found: 88\n",
            "retreiving the corresponding TaxIDs...\n",
            "Total number of results: 88\n",
            "Downloading the CDS fasta files...\n",
            "number of remaining accession numbers with sequence to be found in GenBank files: 45\n",
            "Downloading the GenBank files...\n",
            "number of unique results from NCBI:                                                          88\n",
            "number of genes found in the cds_fasta file:                                                43\n",
            "number of genes found in the genbank file:                                                  1\n",
            "total number of sequences retrieved:                                                        44\n",
            "number with more than one sequences:                                                        0\n",
            "total number of accession version identifiers analysed:                                     88\n",
            "ended:                                                                                      2023-03-07_23-04-32\n",
            "total number of accession version identifiers \n",
            "for which no gene has been retrieved:                                                  44\n",
            "see the notfound.txt for the detail\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import os\n",
        "import argparse             #parsing command line arguments\n",
        "from datetime import datetime    \n",
        "\n",
        "\n",
        "#################################################\n",
        "#############   GLOBAL VARIABLES    #############\n",
        "#################################################\n",
        "\n",
        "# list the selected option to make it appear in report.txt\n",
        "options_report = []\n",
        "\n",
        "#taxa list\n",
        "if len(taxa_lists) > 0:\n",
        "    input_files = ' '.join(taxa_lists)\n",
        "    options_report.append(f\" -list (-L) {input_files}\")\n",
        "\n",
        "    # Check if files exists\n",
        "    for file in taxa_lists:\n",
        "        if not os.path.exists(file):\n",
        "            sys.exit(f\"The file {file} cannot be found\")\n",
        "        if file[-4:] != \".txt\":\n",
        "            sys.exit(f\"The list of taxa {file} must be a .txt file\")\n",
        "\n",
        "#list of chosen options to display in the report.tsv\n",
        "##parse options\n",
        "if tsv:\n",
        "    options_report.append(\"--tsv (-t)\")\n",
        "if information:\n",
        "    options_report.append(\"--information (-i)\")\n",
        "if taxids:\n",
        "    options_report.append(\"--taxids (-T)\")\n",
        "if cds:\n",
        "    options_report.append(f\"--cds (-c) {' '.join(filter)}\")\n",
        "if apikey:\n",
        "    options_report.append(f\"--apikey (-a) {apikey[0]}\")\n",
        "\n",
        "#verbose\n",
        "if verbose:\n",
        "    verb = 2\n",
        "    options_report.append(\"--verbose (-v)\")\n",
        "elif quiet:\n",
        "    verb = 0\n",
        "    options_report.append(\"--quiet (-q)\")\n",
        "else:\n",
        "    verb = 1\n",
        "\n",
        "if cds:\n",
        "  cds = filter\n",
        "\n",
        "OPTIONS = (verb, filter, classif, taxids, tsv, information)\n",
        "\n",
        "##foldername and path\n",
        "starting_time = str(datetime.now())\n",
        "starting_time = '_'.join(starting_time.split())[:19]\n",
        "starting_time = starting_time.replace(\":\", \"-\")\n",
        "path = \"./NSDPY results/\" + starting_time\n",
        "\n",
        "\n",
        "##############################################\n",
        "#########  RUN THE RUN!!  ####################\n",
        "##############################################\n",
        "\n",
        "# Create the directory to store the results\n",
        "if not os.path.exists(path):\n",
        "    os.makedirs(path)\n",
        "\n",
        "# Read the taxa list files \n",
        "if taxa_lists:\n",
        "    taxa_list = []\n",
        "    for file in taxa_lists:\n",
        "        with open(file, \"r\") as data:\n",
        "            taxa_list = taxa_list + data.read().splitlines()\n",
        "    \n",
        "    # Add the taxa to the query\n",
        "    taxa_list = [ taxon + \"[ORGN] OR \" for taxon in taxa_list]\n",
        "\n",
        "    # Base URL with params\n",
        "    esearch_address = 'https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi'\n",
        "    base_URL_length = len(esearch_address) + 100 # Keep 100 chars for params\n",
        "\n",
        "    # Base query\n",
        "    base_query = request + \" AND (\"\n",
        "    \n",
        "    # Remaining space for the taxa list \n",
        "    taxa_max_length = 2048 - ( len(base_query) + base_URL_length )\n",
        "\n",
        "    # Include taxa in the QUERY and make a list of queries <= 2048 chars\n",
        "    remaining_space = taxa_max_length\n",
        "    queries_list = []\n",
        "    new_query = base_query\n",
        "\n",
        "    for taxon in taxa_list:\n",
        "        if (remaining_space - len(taxon) + 4) <= 0:\n",
        "            # Delete the last \"[ORGN] OR \" and close parenthesis\n",
        "            queries_list.append(new_query[:-8] + ')')\n",
        "            # Start another query\n",
        "            new_query = base_query\n",
        "            remaining_space = taxa_max_length\n",
        "        else:\n",
        "            remaining_space = remaining_space - len(taxon)\n",
        "            new_query = new_query + taxon\n",
        "            if queries_list:\n",
        "                queries_list[-1] = new_query\n",
        "            else:\n",
        "                queries_list.append(new_query)\n",
        "    queries_list[-1] = queries_list[-1][:-4] + ')'\n",
        "else:\n",
        "    queries_list = [request]    \n",
        "\n",
        "### Retrieving results from esearch and the related TaxIDs\n",
        "dict_ids = {}\n",
        "\n",
        "for query in queries_list:\n",
        "    query = query.rstrip('0').rstrip(' OR ') + ')'\n",
        "    QUERY = (query, apikey)\n",
        "    if verb != 0:\n",
        "        print(f\"retrieving results for {query} ....\")        \n",
        "\n",
        "    ## esearchquery\n",
        "    y = esearchquery(QUERY)\n",
        "\n",
        "    ##check errors (if bad API key etc) errors returned by the Entrez API\n",
        "    if \"error\" in y.keys():\n",
        "        errors = y[\"error\"]\n",
        "        sys.exit(errors)\n",
        "\n",
        "    count = int(y[\"esearchresult\"][\"count\"])\n",
        "    #comments\n",
        "    if verb > 0:    \n",
        "        print(f'Number of results found: {count}')\n",
        "\n",
        "    if count < 1:\n",
        "        continue\n",
        "    webenv =  str(y[\"esearchresult\"][\"webenv\"])\n",
        "    querykey = str(y[\"esearchresult\"][\"querykey\"])\n",
        "    params = (querykey, webenv, count)\n",
        "\n",
        "    ###Taxids\n",
        "    if verb != 0:\n",
        "        print(\"retreiving the corresponding TaxIDs...\")\n",
        "\n",
        "    subdictids = taxids(params, path, OPTIONS)\n",
        "    dict_ids = {**dict_ids, **subdictids}\n",
        "\n",
        "    total_number_of_results = len(set(dict_ids.keys()))\n",
        "\n",
        "if total_number_of_results < 1: \n",
        "    sys.exit(\"No results found\")\n",
        "\n",
        "if verb != 0:\n",
        "    print(f\"Total number of results: {total_number_of_results}\")\n",
        "\n",
        "#make a set of TaxIDs\n",
        "list_of_ids = list(dict_ids.keys())\n",
        "reverse = set(dict_ids.values())\n",
        "list_of_TaxIDs = list(reverse)\n",
        "\n",
        "\n",
        "### completetaxo (call EFETCH to query the taxonomy database)\n",
        "# Check that an option that requires the taxonomic information has been selected\n",
        "dict_taxo = {}\n",
        "if classif != 3 or information:\n",
        "    dict_taxo = completetaxo(list_of_TaxIDs, QUERY, OPTIONS)\n",
        "\n",
        "\n",
        "### Download the sequences (call to EFETCH to query the nuccore database)\n",
        "if cds is None:\n",
        "    found = fasta(path, dict_ids, dict_taxo, QUERY, list_of_ids, OPTIONS)\n",
        "else:\n",
        "    found = cds_fasta(path, dict_ids, dict_taxo, QUERY, list_of_ids, OPTIONS)\n",
        "\n",
        "### List the remaining access ids:\n",
        "remaining = set(list_of_ids) - set(found)\n",
        "remaining = list(remaining)\n",
        "\n",
        "# Comments\n",
        "if verb > 0 and cds is not None:\n",
        "    print(f'number of remaining accession numbers with sequence to be found in GenBank files: {len(remaining)}')\n",
        "\n",
        "\n",
        "### taxo (call EFETCH to query the nuccore database to get the .gb files) \n",
        "analyse, sequences = taxo(path, remaining, dict_ids, QUERY, dict_taxo, OPTIONS)\n",
        "\n",
        "\n",
        "### summarise\n",
        "# Get the ending time of the run\n",
        "ending_time= str(datetime.now())\n",
        "ending_time= '_'.join(ending_time.split())[:19]\n",
        "ending_time= ending_time.replace(\":\", \"-\")        \n",
        "\n",
        "### Comments\n",
        "genes = found + sequences\n",
        "total = list(set(analyse) | set(found)) \n",
        "notfound = list(set(list_of_ids) - (set(sequences) | set(found)))\n",
        "if cds is not None:\n",
        "    if verb > 0:\n",
        "        print(f'number of unique results from NCBI:                                                          {count}')\n",
        "        print(f'number of genes found in the cds_fasta file:                                                {len(found)}')\n",
        "        print(f'number of genes found in the genbank file:                                                  {len(sequences)}')\n",
        "        print(f'total number of sequences retrieved:                                                        {len(genes)}')\n",
        "        print(f'number with more than one sequences:                                                        {duplicates(genes, path)}')\n",
        "        print(f'total number of accession version identifiers analysed:                                     {len(total)}')\n",
        "        print(f'ended:                                                                                      {ending_time}')\n",
        "    if notfound and verb > 0:\n",
        "        print(f\"total number of accession version identifiers \\nfor which no gene has been retrieved:                                                  {len(notfound)}\")\n",
        "        print(\"see the notfound.txt for the detail\")\n",
        "\n",
        "else:\n",
        "    if verb > 0:\n",
        "        print(f'number of unique results from NCBI:                                  {total_number_of_results}')\n",
        "        print(f'total number of sequences retrieved:                                {len(genes)}')\n",
        "        print(f'number with more than one sequences:                                {duplicates(genes, path)}')\n",
        "        print(f'total number of accession version identifiers analysed:             {len(total)}')\n",
        "        print(f'ended:                                                              {ending_time}')\n",
        "        print(f'number of accession version identifiers analysed with no sequences downloaded:             {len(notfound)}')\n",
        "        if len(notfound) > 0:\n",
        "            print(f'see \"notfound.txt\"')\n",
        "\n",
        "##write summary\n",
        "if cds is None:\n",
        "    filters = \"\"\n",
        "    filetype = \"fasta\"\n",
        "else:\n",
        "    filters = \",\".join(filter)\n",
        "    filetype = \"cds_fasta\"\n",
        "\n",
        "options_report = \",\".join(options_report)\n",
        "\n",
        "try:\n",
        "    y = open(\"report.tsv\")\n",
        "    y.close()\n",
        "    with open(\"report.tsv\", 'a') as r:\n",
        "        r.write(f\"{request}\\t{options_report}\\t{starting_time}\\t{ending_time}\\t{filetype}\\t{count}\\t{filters}\\t{len(found)}\\t{len(list_of_TaxIDs)}\\n\")\n",
        "except:\n",
        "    with open(\"report.tsv\", 'a') as r:\n",
        "        r.write(f\"request\\toptions\\tstarting_time\\tending_time\\tresults type\\tesearch\\tGene filters\\tsequences\\tTaxIDs\\n\")\n",
        "        r.write(f\"{request}\\t{options_report}\\t{starting_time}\\t{ending_time}\\t{filetype}\\t{count}\\t{filters}\\t{len(found)}\\t{len(list_of_TaxIDs)}\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MocqVDKPLnr-"
      },
      "source": [
        "##Archive the results and send to Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Im8zLQ3ry-kk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6a75b54-6332-4129-bfb0-f1f5043497f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory /content/drive/MyDrive/'NSDPY results': File exists\n",
            "env: path=path\n",
            "\tzip warning: name not matched: /content/results\n",
            "\n",
            "zip error: Nothing to do! (try: zip -r /content/drive/MyDrive/NSDPY results/2023-03-07_23-04-34.zip . -i /content/results)\n",
            "Archive saved to /content/drive/MyDrive/NSDPY results/2023-03-07_23-04-34.zip\n"
          ]
        }
      ],
      "source": [
        "##file name \n",
        "name = str(datetime.now())\n",
        "name = '_'.join(name.split())[:19]\n",
        "name = name.replace(\":\", \"-\")\n",
        "path = \"NSDPY results/\" + name\n",
        "\n",
        "!mkdir \"/content/drive/MyDrive/'NSDPY results'\"\n",
        "##check if a google drive variable exists and is set to 1\n",
        "try:\n",
        "  if googledrive == 1:\n",
        "    path = \"/content/drive/MyDrive/\" + path + \".zip\"\n",
        "except NameError:\n",
        "  print(\"googledrive variable does not exist: Google Drive code cell might\\\n",
        "  be missing/modified or hasn't been run  \")\n",
        "  path = \"/content/\" + path + \".zip\"\n",
        "\n",
        "##export the path variable to bash\n",
        "%env path = path\n",
        "\n",
        "##call the zip function from bash\n",
        "!zip -r \"$path\" \"/content/results\"\n",
        "\n",
        "##report\n",
        "print(f\"Archive saved to {path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3695f107-43a5-41a3-8b3f-b40c5172e939",
        "id": "me3fI2OeSmXx"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory /content/drive/MyDrive/'NSDPY results': File exists\n",
            "env: path=path\n",
            "\tzip warning: name not matched: /content/results\n",
            "\n",
            "zip error: Nothing to do! (try: zip -r /content/drive/MyDrive/NSDPY results/2023-03-07_23-00-37.zip . -i /content/results)\n",
            "Archive saved to /content/drive/MyDrive/NSDPY results/2023-03-07_23-00-37.zip\n"
          ]
        }
      ],
      "source": [
        "##file name \n",
        "name = str(datetime.now())\n",
        "name = '_'.join(name.split())[:19]\n",
        "name = name.replace(\":\", \"-\")\n",
        "path = \"NSDPY results/\" + name\n",
        "\n",
        "!mkdir \"/content/drive/MyDrive/'NSDPY results'\"\n",
        "##check if a google drive variable exists and is set to 1\n",
        "try:\n",
        "  if googledrive == 1:\n",
        "    path = \"/content/drive/MyDrive/\" + path + \".zip\"\n",
        "except NameError:\n",
        "  print(\"googledrive variable does not exist: Google Drive code cell might\\\n",
        "  be missing/modified or hasn't been run  \")\n",
        "  path = \"/content/\" + path + \".zip\"\n",
        "\n",
        "##export the path variable to bash\n",
        "%env path = path\n",
        "\n",
        "##call the zip function from bash\n",
        "!zip -r \"$path\" \"/content/results\"\n",
        "\n",
        "##report\n",
        "print(f\"Archive saved to {path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fbdf6a46-57ff-4dac-ac05-f55280950bf5",
        "id": "RP8xzuzESnzL"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory /content/drive/MyDrive/'NSDPY results': File exists\n",
            "env: path=path\n",
            "\tzip warning: name not matched: /content/results\n",
            "\n",
            "zip error: Nothing to do! (try: zip -r /content/drive/MyDrive/NSDPY results/2023-03-07_23-00-37.zip . -i /content/results)\n",
            "Archive saved to /content/drive/MyDrive/NSDPY results/2023-03-07_23-00-37.zip\n"
          ]
        }
      ],
      "source": [
        "##file name \n",
        "name = str(datetime.now())\n",
        "name = '_'.join(name.split())[:19]\n",
        "name = name.replace(\":\", \"-\")\n",
        "path = \"NSDPY results/\" + name\n",
        "\n",
        "!mkdir \"/content/drive/MyDrive/'NSDPY results'\"\n",
        "##check if a google drive variable exists and is set to 1\n",
        "try:\n",
        "  if googledrive == 1:\n",
        "    path = \"/content/drive/MyDrive/\" + path + \".zip\"\n",
        "except NameError:\n",
        "  print(\"googledrive variable does not exist: Google Drive code cell might\\\n",
        "  be missing/modified or hasn't been run  \")\n",
        "  path = \"/content/\" + path + \".zip\"\n",
        "\n",
        "##export the path variable to bash\n",
        "%env path = path\n",
        "\n",
        "##call the zip function from bash\n",
        "!zip -r \"$path\" \"/content/results\"\n",
        "\n",
        "##report\n",
        "print(f\"Archive saved to {path}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}